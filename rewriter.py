import sqlite3
import numpy as np
import pandas as pd
from typing import Tuple, Optional, List, Dict, Set
from antlr4 import *
from antlr4.TokenStreamRewriter import TokenStreamRewriter
from antlr4.error.ErrorListener import ErrorListener
import re
import hashlib
import json
from datetime import datetime
from dataclasses import dataclass
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import generated ANTLR files
try:
    from DPDSLLexer import DPDSLLexer
    from DPDSLParser import DPDSLParser
    from DPDSLVisitor import DPDSLVisitor
except ImportError:
    raise ImportError(
        "âŒ CRITICAL: ANTLR files not found!\n"
        "Run: java -jar antlr-4.13.1-complete.jar -Dlanguage=Python3 -visitor DPDSL.g4\n"
        "Then ensure DPDSLLexer.py, DPDSLParser.py, DPDSLVisitor.py are in the same directory."
    )


# === CONFIGURATION ===
class DPDSLConfig:
    """Configuration for DPDSL rewriter"""
    
    # Sensitivity bounds (customize for your data)
    METADATA_BOUNDS = {
        'Salary': 300000,
        'salary': 300000,
        'budget': 1000000,
        'age': 100,
        'hours_worked': 80,
        'medical_cost': 100000,
        'performance_rating': 5,
    }
    
    # Privacy parameters
    DEFAULT_EPSILON = 1.0
    MAX_EPSILON_PER_QUERY = 2.0
    MAX_BUDGET_PER_SESSION = 10.0
    
    # Elastic sensitivity for JOINs
    MAX_CONTRIBUTIONS = 3
    ELASTIC_SENSITIVITY_ENABLED = True
    
    # HIPAA: PII columns that CANNOT be accessed directly
    PROHIBITED_COLUMNS = {
        'email', 'Email', 'EMAIL',
        'ssn', 'SSN', 'social_security_number',
        'address', 'Address', 'ADDRESS',
        'phone', 'Phone', 'phone_number', 'PHONE',
        'bank_account', 'Bank_account_number', 'bank_account_number',
        'first_name', 'First_name', 'FirstName', 'FIRST_NAME',
        'last_name', 'Last_name', 'LastName', 'LAST_NAME',
        'zip', 'Zip', 'ZIP', 'zipcode', 'postal_code',
        'date_of_birth', 'dob', 'DOB', 'birth_date',
        'medical_record_number', 'patient_id',
    }
    
    # Sensitive columns requiring DP (can be aggregated with PRIVATE label)
    SENSITIVE_COLUMNS = {
        'salary', 'Salary', 'SALARY',
        'budget', 'Budget',
        'performance_rating', 'rating',
        'medical_cost', 'claim_amount',
        'age', 'Age',
    }
    
    # Audit logging
    AUDIT_LOG_FILE = 'dpdsl_audit.jsonl'
    ENABLE_AUDIT_LOGGING = True


# === HIPAA COMPLIANCE CHECKER ===
class HIPAAComplianceChecker:
    """
    Validates queries against HIPAA regulations.
    Blocks direct access to PII and ensures proper aggregation.
    """
    
    def __init__(self, config: DPDSLConfig):
        self.config = config
        self.prohibited_cols = config.PROHIBITED_COLUMNS
    
    def check_query(self, query: str) -> Tuple[bool, Optional[str]]:
        """
        Returns: (is_compliant, error_message)
        """
        query_upper = query.upper()
        
        # Extract all column references from SELECT clause
        columns = self._extract_selected_columns(query)
        
        # Check for direct PII access
        for col in columns:
            if col in self.prohibited_cols:
                return False, f"ðŸš« HIPAA VIOLATION: Direct access to PII column '{col}' is prohibited"
        
        # Check for ORDER BY on sensitive columns with LIMIT
        if self._has_risky_order_by_limit(query):
            return False, "ðŸš« BLOCKED: ORDER BY with LIMIT could enable re-identification of individuals"
        
        # Check for LIMIT 1 on non-aggregated queries
        if self._has_limit_one_without_aggregation(query):
            return False, "ðŸš« BLOCKED: LIMIT 1 without aggregation could identify specific individuals"
        
        return True, None
    
    def _extract_selected_columns(self, query: str) -> Set[str]:
        """Extract column names from SELECT clause"""
        columns = set()
        
        # Remove labels and extract column names
        query_clean = re.sub(r'\b(PRIVATE|PUBLIC)\s+', '', query, flags=re.IGNORECASE)
        
        # Find SELECT clause
        select_match = re.search(r'SELECT\s+(.*?)\s+FROM', query_clean, re.IGNORECASE | re.DOTALL)
        if not select_match:
            return columns
        
        select_clause = select_match.group(1)
        
        # Skip if it's COUNT(*) or similar aggregations
        if re.search(r'COUNT\s*\(\s*\*\s*\)', select_clause, re.IGNORECASE):
            return columns
        
        # Extract column names (simple pattern matching)
        # Handles: col, table.col, AVG(col), MIN(col, bound)
        col_pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]*)\b'
        potential_cols = re.findall(col_pattern, select_clause)
        
        # Filter out SQL keywords and function names
        keywords = {'SELECT', 'FROM', 'WHERE', 'GROUP', 'BY', 'AVG', 'SUM', 'COUNT', 'MIN', 'MAX', 'AS', 'OF'}
        columns = {col for col in potential_cols if col.upper() not in keywords}
        
        return columns
    
    def _has_risky_order_by_limit(self, query: str) -> bool:
        """Check for ORDER BY with small LIMIT"""
        query_upper = query.upper()
        if 'ORDER BY' in query_upper and 'LIMIT' in query_upper:
            # Extract LIMIT value
            limit_match = re.search(r'LIMIT\s+(\d+)', query, re.IGNORECASE)
            if limit_match and int(limit_match.group(1)) <= 10:
                return True
        return False
    
    def _has_limit_one_without_aggregation(self, query: str) -> bool:
        """Check for LIMIT 1 on non-aggregated queries"""
        query_upper = query.upper()
        
        # Check if LIMIT 1 exists
        if not re.search(r'LIMIT\s+1\b', query_upper):
            return False
        
        # Check if there's aggregation
        has_aggregation = bool(re.search(r'\b(AVG|SUM|COUNT|MIN|MAX)\s*\(', query_upper))
        
        return not has_aggregation


# === ELASTIC SENSITIVITY FOR JOINS ===
@dataclass
class JoinPath:
    tables: List[str]
    join_conditions: List[Tuple[str, str]]
    primary_entity_table: str
    
    def get_entity_column(self) -> str:
        return f"{self.primary_entity_table}.id"


class MultiTableJoinAnalyzer:
    def __init__(self):
        self.primary_entity_hints = ['employee', 'user', 'person', 'patient', 'customer']
    
    def analyze_query(self, query_text: str) -> Optional[JoinPath]:
        query_upper = query_text.upper()
        if 'JOIN' not in query_upper:
            return None
        
        tables = []
        join_conditions = []
        
        # Extract FROM table
        from_match = re.search(r'FROM\s+(\w+)(?:\s+(?:AS\s+)?(\w+))?', query_text, re.IGNORECASE)
        if from_match:
            tables.append(from_match.group(1))
        
        # Extract JOIN tables
        join_pattern = r'JOIN\s+(\w+)(?:\s+(?:AS\s+)?(\w+))?\s+ON\s+([\w.]+)\s*=\s*([\w.]+)'
        for match in re.finditer(join_pattern, query_text, re.IGNORECASE):
            tables.append(match.group(1))
            join_conditions.append((match.group(3), match.group(4)))
        
        if len(tables) < 2:
            return None
        
        primary_entity = self._identify_primary_entity(tables)
        return JoinPath(tables=tables, join_conditions=join_conditions, 
                       primary_entity_table=primary_entity)
    
    def _identify_primary_entity(self, tables: List[str]) -> str:
        for table in tables:
            table_lower = table.lower()
            for hint in self.primary_entity_hints:
                if hint in table_lower:
                    return table
        return tables[0]


class ElasticSensitivityManager:
    def __init__(self, max_contributions=3):
        self.max_contributions = max_contributions
    
    def apply_elastic_clipping(self, df: pd.DataFrame, join_path: JoinPath, 
                               verbose=False) -> pd.DataFrame:
        entity_col = self._find_entity_column(df, join_path)
        if entity_col is None:
            if verbose:
                logger.warning("Could not find entity column for elastic clipping")
            return df
        
        initial_rows = len(df)
        df_with_counts = df.copy()
        df_with_counts['_contribution_count'] = df_with_counts.groupby(entity_col).cumcount() + 1
        df_clipped = df_with_counts[df_with_counts['_contribution_count'] <= self.max_contributions].copy()
        df_clipped = df_clipped.drop('_contribution_count', axis=1)
        
        if verbose:
            rows_removed = initial_rows - len(df_clipped)
            logger.info(f"Elastic clipping: {rows_removed} rows suppressed")
        
        return df_clipped
    
    def _find_entity_column(self, df: pd.DataFrame, join_path: JoinPath) -> Optional[str]:
        possible_names = ['id', f"{join_path.primary_entity_table}.id", 
                         f"{join_path.primary_entity_table}_id", 'employee_id', 'user_id']
        for name in possible_names:
            if name in df.columns:
                return name
        return None
    
    def calculate_sensitivity(self, base_sensitivity: float) -> float:
        return base_sensitivity * self.max_contributions


# === BUDGET MANAGER ===
class BudgetManager:
    def __init__(self, max_budget=10.0):
        self.max_budget = max_budget
        self.current_spent = 0.0
        self.query_log = []
    
    def check_affordability(self, cost: float) -> bool:
        return (self.current_spent + cost) <= self.max_budget
    
    def spend(self, cost: float, query_text: str) -> float:
        if not self.check_affordability(cost):
            raise ValueError(
                f"â›” BUDGET EXHAUSTED: Query needs Îµ={cost:.2f}, "
                f"but only Îµ={self.remaining():.2f} remains"
            )
        self.current_spent += cost
        self.query_log.append({
            'query': query_text[:100],
            'cost': cost, 
            'remaining': self.remaining(),
            'timestamp': datetime.now().isoformat()
        })
        return self.remaining()
    
    def remaining(self) -> float:
        return max(0.0, self.max_budget - self.current_spent)


# === AUDIT LOGGER ===
class AuditLogger:
    def __init__(self, log_file: str = DPDSLConfig.AUDIT_LOG_FILE):
        self.log_file = log_file
        self.enabled = DPDSLConfig.ENABLE_AUDIT_LOGGING
    
    def log_query(self, user_id: str, query: str, result: str, 
                  privacy_cost: float, blocked: bool, reason: Optional[str] = None):
        if not self.enabled:
            return
        
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'user_id': user_id,
            'query': query[:200],  # Truncate long queries
            'query_hash': hashlib.sha256(query.encode()).hexdigest()[:16],
            'result': result[:100],  # Truncate long results
            'privacy_cost': privacy_cost,
            'blocked': blocked,
            'reason': reason
        }
        
        try:
            with open(self.log_file, 'a') as f:
                f.write(json.dumps(log_entry) + '\n')
        except Exception as e:
            logger.error(f"Failed to write audit log: {e}")


# === SYNTAX ERROR LISTENER ===
class SyntaxErrorListener(ErrorListener):
    def __init__(self):
        super().__init__()
        self.errors = []
    
    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        self.errors.append(f"Syntax error at line {line}:{column} - {msg}")


# === DPDSL REWRITER VISITOR ===
class DPDSL_Rewriter_Visitor(DPDSLVisitor):
    def __init__(self, token_stream, original_query="", config: DPDSLConfig = None):
        self.token_stream = token_stream
        self.rewriter = TokenStreamRewriter(token_stream)
        self.privacy_cost = 0.0
        self.errors = []
        self.has_group_by = False
        self.original_query = original_query
        self.config = config or DPDSLConfig()
        self.join_analyzer = MultiTableJoinAnalyzer()
        self.elastic_manager = ElasticSensitivityManager(
            max_contributions=self.config.MAX_CONTRIBUTIONS
        )
        self.join_path = None
        self.has_join = False
    
    def visitJoin_clause(self, ctx):
        self.has_join = True
        if self.join_path is None and self.original_query:
            self.join_path = self.join_analyzer.analyze_query(self.original_query)
        return self.visitChildren(ctx)
    
    def visitAggregation(self, ctx):
        inner_expr = ctx.expression()
        
        if type(inner_expr).__name__ == 'LabeledColumnContext':
            label_ctx = inner_expr.label()
            if label_ctx:
                label = label_ctx.getText()
                
                if label == 'PRIVATE':
                    col_name = self._extract_column_name(inner_expr)
                    base_sensitivity = self.config.METADATA_BOUNDS.get(col_name, 100000)
                    
                    # Apply elastic sensitivity for JOINs
                    if self.has_join and self.join_path and self.config.ELASTIC_SENSITIVITY_ENABLED:
                        sensitivity = self.elastic_manager.calculate_sensitivity(base_sensitivity)
                    else:
                        sensitivity = base_sensitivity
                    
                    # Extract epsilon budget
                    budget = self.config.DEFAULT_EPSILON
                    if ctx.budget():
                        budget_text = ctx.budget().getText()
                        budget_match = re.search(r'[\d.]+', budget_text)
                        if budget_match:
                            budget = float(budget_match.group())
                        
                        if budget > self.config.MAX_EPSILON_PER_QUERY:
                            self.errors.append(
                                f"Epsilon {budget} exceeds maximum allowed {self.config.MAX_EPSILON_PER_QUERY}"
                            )
                            return None
                    
                    # Rewrite: clip column value
                    new_col_sql = f"MIN({col_name}, {base_sensitivity})"
                    self.rewriter.replaceRange(
                        inner_expr.start.tokenIndex, 
                        inner_expr.stop.tokenIndex, 
                        new_col_sql
                    )
                    
                    # Add Laplace noise
                    noise = np.random.laplace(0, sensitivity / budget)
                    self.rewriter.insertAfter(ctx.stop.tokenIndex, f" + {noise:.2f}")
                    self.privacy_cost += budget
                    
                elif label == 'PUBLIC':
                    col_name = self._extract_column_name(inner_expr)
                    self.rewriter.replaceRange(
                        inner_expr.start.tokenIndex,
                        inner_expr.stop.tokenIndex,
                        col_name
                    )
        
        return self.visitChildren(ctx)
    
    def visitLabeledColumn(self, ctx):
        # Check if this column is inside an aggregation
        parent = ctx.parentCtx
        while parent:
            if isinstance(parent, DPDSLParser.AggregationContext):
                return self.visitChildren(ctx)
            parent = parent.parentCtx if hasattr(parent, 'parentCtx') else None
        
        if ctx.label():
            label = ctx.label().getText()
            col_name = self._extract_column_name(ctx)
            
            if label == 'PUBLIC':
                self.rewriter.replaceRange(ctx.start.tokenIndex, ctx.stop.tokenIndex, col_name)
            elif label == 'PRIVATE':
                self.errors.append(
                    f"ERROR: PRIVATE column '{col_name}' cannot be selected directly. "
                    f"Use aggregation with DP noise: AVG(PRIVATE {col_name} OF [Îµ])"
                )
        
        return self.visitChildren(ctx)
    
    def visitGroup_by_clause(self, ctx):
        self.has_group_by = True
        for col_ctx in ctx.groupByColumn():
            if col_ctx.label() and col_ctx.label().getText() == 'PRIVATE':
                col_name = self._extract_column_name(col_ctx)
                self.errors.append(
                    f"ERROR: GROUP BY on PRIVATE column '{col_name}' is not allowed"
                )
            elif col_ctx.label() and col_ctx.label().getText() == 'PUBLIC':
                col_name = self._extract_column_name(col_ctx)
                self.rewriter.replaceRange(
                    col_ctx.start.tokenIndex,
                    col_ctx.stop.tokenIndex,
                    col_name
                )
        return self.visitChildren(ctx)
    
    def _extract_column_name(self, ctx):
        if hasattr(ctx, 'identifier') and ctx.identifier():
            return ctx.identifier().getText()
        elif hasattr(ctx, 'table_column'):
            tc = ctx.table_column()
            if tc.identifier(1):
                return tc.identifier(1).getText()
            return tc.identifier(0).getText()
        return "unknown"
    
    def get_rewritten_sql(self) -> str:
        sql = self.rewriter.getDefaultText()
        
        # Remove DP annotations
        sql = re.sub(r'OF\s*\[[^\]]+\]', '', sql)
        sql = re.sub(r'\bPRIVATE\s*', '', sql)
        sql = re.sub(r'\bPUBLIC\s*', '', sql)
        
        # Clean up spacing
        sql = re.sub(r'\s+', ' ', sql)
        sql = re.sub(r',(?!\s)', ', ', sql)
        
        return sql.strip()


# === MAIN REWRITER CLASS ===
class DPDSLRewriter:
    """
    Production-ready DPDSL Rewriter with full security features.
    
    Usage:
        rewriter = DPDSLRewriter(db_connection)
        result, error = rewriter.execute_query(query, user_id)
    """
    
    def __init__(self, db_connection: sqlite3.Connection, config: DPDSLConfig = None):
        self.conn = db_connection
        self.config = config or DPDSLConfig()
        self.hipaa_checker = HIPAAComplianceChecker(self.config)
        self.audit_logger = AuditLogger()
        self.user_budgets = {}
        
        logger.info("âœ… DPDSL Rewriter initialized")
        logger.info(f"   Max budget per session: Îµ = {self.config.MAX_BUDGET_PER_SESSION}")
        logger.info(f"   HIPAA protection: {len(self.config.PROHIBITED_COLUMNS)} PII columns blocked")
    
    def execute_query(self, query: str, user_id: str, verbose: bool = False) -> Tuple[Optional[List], Optional[str]]:
        """
        Execute query with full DP protection and HIPAA compliance.
        
        Returns: (results, error_message)
        """
        
        # Step 1: HIPAA Compliance Check
        is_compliant, hipaa_error = self.hipaa_checker.check_query(query)
        if not is_compliant:
            self.audit_logger.log_query(user_id, query, 'HIPAA_VIOLATION', 0.0, True, hipaa_error)
            return None, hipaa_error
        
        # Step 2: Get/create user budget
        if user_id not in self.user_budgets:
            self.user_budgets[user_id] = BudgetManager(max_budget=self.config.MAX_BUDGET_PER_SESSION)
        budget_manager = self.user_budgets[user_id]
        
        try:
            # Step 3: Parse query
            input_stream = InputStream(query)
            lexer = DPDSLLexer(input_stream)
            lexer.removeErrorListeners()
            lexer_error_listener = SyntaxErrorListener()
            lexer.addErrorListener(lexer_error_listener)
            
            stream = CommonTokenStream(lexer)
            parser = DPDSLParser(stream)
            parser.removeErrorListeners()
            parser_error_listener = SyntaxErrorListener()
            parser.addErrorListener(parser_error_listener)
            
            tree = parser.query()
            
            # Check for syntax errors
            all_errors = lexer_error_listener.errors + parser_error_listener.errors
            if all_errors:
                error_msg = all_errors[0]
                self.audit_logger.log_query(user_id, query, 'SYNTAX_ERROR', 0.0, True, error_msg)
                return None, f"âŒ {error_msg}"
            
            # Step 4: Rewrite with DP
            visitor = DPDSL_Rewriter_Visitor(stream, original_query=query, config=self.config)
            visitor.visit(tree)
            
            if visitor.errors:
                error_msg = visitor.errors[0]
                self.audit_logger.log_query(user_id, query, 'VALIDATION_ERROR', 0.0, True, error_msg)
                return None, f"âŒ {error_msg}"
            
            # Step 5: Check budget
            if visitor.privacy_cost > 0:
                if not budget_manager.check_affordability(visitor.privacy_cost):
                    error_msg = f"â›” BUDGET EXHAUSTED: need Îµ={visitor.privacy_cost:.2f}, have Îµ={budget_manager.remaining():.2f}"
                    self.audit_logger.log_query(user_id, query, 'BUDGET_EXCEEDED', 0.0, True, error_msg)
                    return None, error_msg
            
            # Step 6: Get rewritten SQL
            final_sql = visitor.get_rewritten_sql()
            
            if verbose:
                logger.info(f"Original:  {query}")
                logger.info(f"Rewritten: {final_sql}")
                logger.info(f"Privacy cost: Îµ = {visitor.privacy_cost}")
            
            # Step 7: Execute
            if visitor.has_join and visitor.join_path and self.config.ELASTIC_SENSITIVITY_ENABLED:
                df = pd.read_sql_query(final_sql, self.conn)
                df = visitor.elastic_manager.apply_elastic_clipping(df, visitor.join_path, verbose)
                result = [tuple(row) for row in df.values.tolist()] if len(df) > 0 else []
            else:
                cursor = self.conn.cursor()
                cursor.execute(final_sql)
                result = cursor.fetchall()
            
            # Step 8: Spend budget
            if visitor.privacy_cost > 0:
                budget_manager.spend(visitor.privacy_cost, query)
            
            # Step 9: Audit log
            self.audit_logger.log_query(
                user_id, query, f"{len(result)} rows", 
                visitor.privacy_cost, False, None
            )
            
            return result, None
            
        except ValueError as e:
            # Budget or validation errors
            self.audit_logger.log_query(user_id, query, 'ERROR', 0.0, True, str(e))
            return None, str(e)
        except sqlite3.Error as e:
            # Database errors - sanitize message
            error_msg = "âŒ Database error: query execution failed"
            self.audit_logger.log_query(user_id, query, 'DB_ERROR', 0.0, True, error_msg)
            logger.error(f"Database error for user {user_id}: {e}")
            return None, error_msg
        except Exception as e:
            # Unexpected errors - sanitize
            error_msg = "âŒ Internal error: query processing failed"
            self.audit_logger.log_query(user_id, query, 'INTERNAL_ERROR', 0.0, True, error_msg)
            logger.error(f"Unexpected error for user {user_id}: {e}", exc_info=True)
            return None, error_msg
    
    def get_user_budget_status(self, user_id: str) -> Dict:
        """Get budget information for a user"""
        if user_id not in self.user_budgets:
            return {
                'remaining': self.config.MAX_BUDGET_PER_SESSION,
                'spent': 0.0,
                'max': self.config.MAX_BUDGET_PER_SESSION,
                'queries': 0
            }
        
        budget = self.user_budgets[user_id]
        return {
            'remaining': budget.remaining(),
            'spent': budget.current_spent,
            'max': budget.max_budget,
            'queries': len(budget.query_log)
        }
    
    def reset_user_budget(self, user_id: str):
        """Reset budget for a user (new session)"""
        if user_id in self.user_budgets:
            del self.user_budgets[user_id]
            logger.info(f"Budget reset for user {user_id}")


# === FACTORY FUNCTION ===
def create_rewriter(database_path: str = None, 
                   db_connection: sqlite3.Connection = None,
                   config: DPDSLConfig = None) -> DPDSLRewriter:
    """
    Create a configured DPDSLRewriter instance.
    
    Args:
        database_path: Path to SQLite database
        db_connection: Existing database connection
        config: Custom configuration
        
    Returns:
        DPDSLRewriter instance
    """
    if db_connection:
        conn = db_connection
    elif database_path:
        conn = sqlite3.connect(database_path)
    else:
        raise ValueError("Must provide either database_path or db_connection")
    
    return DPDSLRewriter(conn, config)


if __name__ == "__main__":
    print("=" * 70)
    print("DPDSL REWRITER - PRODUCTION READY")
    print("=" * 70)
    print()
    print("âœ… All critical security issues fixed:")
    print("   â€¢ HIPAA PII column protection")
    print("   â€¢ ORDER BY + LIMIT restrictions")
    print("   â€¢ Sanitized error messages")
    print("   â€¢ Comprehensive validation pipeline")
    print()
    print("Usage:")
    print("   from dpdsl_rewriter_fixed import create_rewriter")
    print("   rewriter = create_rewriter(database_path='hospital.db')")
    print("   result, error = rewriter.execute_query(query, user_id)")
    print("=" * 70)