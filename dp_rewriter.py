
import unittest
import sqlite3
import numpy as np
from faker import Faker
import random
from antlr4 import *
from antlr4.TokenStreamRewriter import TokenStreamRewriter
from antlr4.error.ErrorListener import ErrorListener
import re
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
# --- CONFIGURATION ---
METADATA_BOUNDS = {
    'salary': 150000,
    'age': 100,
    'medical_cost': 50000,
    'budget': 50000,  # For project budgets
    'hours_worked': 80,
}
DEFAULT_EPSILON = 1.0

# Elastic sensitivity configuration for JOINs
ELASTIC_CONFIG = {
    'max_contributions': 3,  # Max times a person can contribute to aggregation
    'enabled': True,  # Set to False to disable elastic sensitivity
}


# === MULTI-TABLE JOIN SUPPORT ===

@dataclass
class JoinPath:
    """Represents the path of JOINs in a query"""
    tables: List[str]
    join_conditions: List[Tuple[str, str]]
    primary_entity_table: str
    
    def get_entity_column(self) -> str:
        """Get the column that identifies the primary entity"""
        return f"{self.primary_entity_table}.id"
    
    def fanout_risk(self) -> str:
        """Assess fanout risk based on JOIN structure"""
        if len(self.tables) <= 2:
            return "LOW"
        elif len(self.tables) <= 4:
            return "MEDIUM"
        else:
            return "HIGH"


class MultiTableJoinAnalyzer:
    """Analyzes multi-table JOINs to determine primary entity and fanout"""
    
    def __init__(self):
        self.primary_entity_hints = ['employee', 'user', 'person', 'patient', 'customer']
    
    def analyze_query(self, query_text: str) -> Optional[JoinPath]:
        """Extract JOIN structure from query"""
        query_upper = query_text.upper()
        
        if 'JOIN' not in query_upper:
            return None
        
        tables = []
        join_conditions = []
        
        # FROM clause
        from_match = re.search(r'FROM\s+(\w+)(?:\s+AS\s+)?(\w+)?', query_text, re.IGNORECASE)
        if from_match:
            tables.append(from_match.group(1))
        
        # JOIN clauses
        join_pattern = r'JOIN\s+(\w+)(?:\s+AS\s+)?(\w+)?\s+ON\s+([\w.]+)\s*=\s*([\w.]+)'
        for match in re.finditer(join_pattern, query_text, re.IGNORECASE):
            tables.append(match.group(1))
            join_conditions.append((match.group(3), match.group(4)))
        
        if len(tables) < 2:
            return None
        
        primary_entity = self._identify_primary_entity(tables)
        
        return JoinPath(
            tables=tables,
            join_conditions=join_conditions,
            primary_entity_table=primary_entity
        )
    
    def _identify_primary_entity(self, tables: List[str]) -> str:
        """Identify which table contains the primary entity"""
        for table in tables:
            table_lower = table.lower()
            for hint in self.primary_entity_hints:
                if hint in table_lower:
                    return table
        return tables[0]


class ElasticSensitivityManager:
    """Manages elastic sensitivity for multi-table JOINs"""
    
    def __init__(self, max_contributions=3):
        self.max_contributions = max_contributions
        self.contribution_column = '_dp_contribution_count'
        self.entity_id_column = '_dp_entity_id'
    
    def apply_elastic_clipping(self, 
                               df: pd.DataFrame,
                               join_path: JoinPath,
                               verbose=False) -> pd.DataFrame:
        """Apply elastic clipping to limit per-entity contributions"""
        
        # Find entity ID column
        entity_col = self._find_entity_column(df, join_path)
        
        if entity_col is None:
            if verbose:
                print(f"   ‚ö†Ô∏è  Could not identify entity column, skipping elastic clipping")
            return df
        
        initial_rows = len(df)
        
        # Create normalized entity ID
        df[self.entity_id_column] = df[entity_col]
        
        # Count contributions per entity
        df[self.contribution_column] = df.groupby(self.entity_id_column).cumcount() + 1
        
        # Clip to max contributions
        df_clipped = df[df[self.contribution_column] <= self.max_contributions].copy()
        
        if verbose:
            rows_removed = initial_rows - len(df_clipped)
            print(f"   üîß Elastic clipping: {rows_removed} rows suppressed "
                  f"({rows_removed/initial_rows*100:.1f}% of JOIN result)")
            print(f"      Max contributions per entity: {self.max_contributions}")
        
        return df_clipped
    
    def _find_entity_column(self, df: pd.DataFrame, join_path: JoinPath) -> Optional[str]:
        """Find the entity ID column in the dataframe"""
        possible_names = [
            'id',
            f"{join_path.primary_entity_table}.id",
            f"{join_path.primary_entity_table}_id",
        ]
        
        for name in possible_names:
            if name in df.columns:
                return name
        
        # Try to find any column with primary entity name + 'id'
        for col in df.columns:
            if join_path.primary_entity_table.lower() in col.lower() and 'id' in col.lower():
                return col
        
        return None
    
    def calculate_sensitivity(self, base_sensitivity: float) -> float:
        """Calculate effective sensitivity with elastic clipping"""
        return base_sensitivity * self.max_contributions
    
# Import your ANTLR-generated files
try:
    from DPDSLLexer import DPDSLLexer
    from DPDSLParser import DPDSLParser
    from DPDSLVisitor import DPDSLVisitor
except ImportError:
    print("‚ö†Ô∏è WARNING: Run ANTLR generation first!")
    print("Command: java -jar antlr-4.13.1-complete.jar -Dlanguage=Python3 -visitor DPDSL.g4")

# --- CONFIGURATION ---
METADATA_BOUNDS = {
    'salary': 150000,
    'age': 100,
    'medical_cost': 50000
}
DEFAULT_EPSILON = 1.0

# --- BUDGET MANAGER ---
class BudgetManager:
    """Tracks and enforces differential privacy budget across queries"""
    
    def __init__(self, max_budget=10.0):
        self.max_budget = max_budget
        self.current_spent = 0.0
        self.query_log = []
    
    def check_affordability(self, cost):
        """Check if there's enough budget left for this query"""
        return (self.current_spent + cost) <= self.max_budget
    
    def spend(self, cost, query_text):
        """
        Deduct privacy budget for a query.
        Raises ValueError if insufficient budget remains.
        """
        if not self.check_affordability(cost):
            raise ValueError(
                f"‚õî BUDGET EXHAUSTED: Query needs Œµ={cost:.2f}, "
                f"but only Œµ={self.remaining():.2f} remains. "
                f"You've already spent Œµ={self.current_spent:.2f} of Œµ={self.max_budget:.2f} total."
            )
        
        self.current_spent += cost
        self.query_log.append({
            'query': query_text,
            'cost': cost,
            'remaining': self.remaining()
        })
        return self.remaining()
    
    def remaining(self):
        """Return remaining privacy budget"""
        return self.max_budget - self.current_spent
    
    def reset(self):
        """Reset budget (use carefully - loses all privacy guarantees!)"""
        self.current_spent = 0.0
        self.query_log = []
    
    def get_report(self):
        """Generate a budget usage report"""
        report = f"\n{'='*70}\n"
        report += f"üìä PRIVACY BUDGET REPORT\n"
        report += f"{'='*70}\n"
        report += f"Total Budget: Œµ = {self.max_budget:.2f}\n"
        report += f"Used: Œµ = {self.current_spent:.2f} ({self.current_spent/self.max_budget*100:.1f}%)\n"
        report += f"Remaining: Œµ = {self.remaining():.2f} ({self.remaining()/self.max_budget*100:.1f}%)\n"
        report += f"Queries executed: {len(self.query_log)}\n"
        report += f"\n{'‚îÄ'*70}\n"
        report += f"Query History:\n"
        report += f"{'‚îÄ'*70}\n"
        for i, entry in enumerate(self.query_log, 1):
            report += f"{i}. Cost: Œµ={entry['cost']:.2f}, Remaining: Œµ={entry['remaining']:.2f}\n"
            report += f"   Query: {entry['query'][:60]}...\n"
        report += f"{'='*70}\n"
        return report

# --- REWRITER (Fixed and Working) ---
class DPDSL_Rewriter(DPDSLVisitor):
    def __init__(self, token_stream, original_query=""):
        self.token_stream = token_stream
        self.rewriter = TokenStreamRewriter(token_stream)
        self.privacy_cost = 0.0
        self.errors = []
        self.has_group_by = False
        self.private_columns_in_groupby = []
        
        # NEW: JOIN support
        self.original_query = original_query
        self.join_analyzer = MultiTableJoinAnalyzer()
        self.elastic_manager = ElasticSensitivityManager(
            max_contributions=ELASTIC_CONFIG['max_contributions']
        )
        self.join_path = None
        self.has_join = False

    def visitCountStar(self, ctx):
        """Handle COUNT(*) - no privacy rewriting needed"""
        return self.visitChildren(ctx)
    
    def visitJoin_clause(self, ctx):
        """Detect JOIN in query"""
        self.has_join = True
        
        # Analyze the full query structure (only once)
        if self.join_path is None and self.original_query:
            self.join_path = self.join_analyzer.analyze_query(self.original_query)
        
        return self.visitChildren(ctx)

    def visitAggregation(self, ctx):
        """Handle aggregations with PRIVATE label and elastic sensitivity for JOINs"""
        inner_expr = ctx.expression()
        
        # Check if the inner expression is a LabeledColumn
        if type(inner_expr).__name__ == 'LabeledColumnContext':
            label_ctx = inner_expr.label()
            if label_ctx:
                label = label_ctx.getText()
                
                if label == 'PRIVATE':
                    col_name = inner_expr.identifier().getText() if hasattr(inner_expr, 'identifier') else \
                               inner_expr.table_column().identifier(1).getText() if hasattr(inner_expr, 'table_column') and inner_expr.table_column().identifier(1) else \
                               inner_expr.table_column().identifier(0).getText()
                    
                    # Get sensitivity bound
                    base_sensitivity = METADATA_BOUNDS.get(col_name, 100000)
                    
                    # Calculate effective sensitivity (with elastic sensitivity for JOINs)
                    if self.has_join and self.join_path and ELASTIC_CONFIG['enabled']:
                        sensitivity = self.elastic_manager.calculate_sensitivity(base_sensitivity)
                    else:
                        sensitivity = base_sensitivity
                    
                    # Extract budget (epsilon)
                    budget = DEFAULT_EPSILON
                    if ctx.budget():
                        budget_text = ctx.budget().getText()
                        budget = float(re.search(r'[\d.]+', budget_text).group())
                    
                    # REWRITE: Apply clipping using MIN (SQLite doesn't have LEAST)
                    new_col_sql = f"MIN({col_name}, {base_sensitivity})"  # Always clip to base bound
                    self.rewriter.replaceRange(
                        inner_expr.start.tokenIndex, 
                        inner_expr.stop.tokenIndex, 
                        new_col_sql
                    )
                    
                    # Add Laplace noise (using elastic sensitivity if JOIN present)
                    noise = np.random.laplace(0, sensitivity / budget)
                    self.rewriter.insertAfter(ctx.stop.tokenIndex, f" + {noise:.2f}")
                    
                    # Track privacy cost
                    self.privacy_cost += budget
                    
                elif label == 'PUBLIC':
                    # Just replace with column name
                    col_name = inner_expr.identifier().getText() if hasattr(inner_expr, 'identifier') else \
                               inner_expr.table_column().identifier(1).getText() if hasattr(inner_expr, 'table_column') and inner_expr.table_column().identifier(1) else \
                               inner_expr.table_column().identifier(0).getText()
                    self.rewriter.replaceRange(
                        inner_expr.start.tokenIndex,
                        inner_expr.stop.tokenIndex,
                        col_name
                    )
        
        return self.visitChildren(ctx)
    
    def visitLabeledColumn(self, ctx):
        """Handle labeled column references"""
        # Check if we're inside an Aggregation context by walking up the tree
        parent = ctx.parentCtx
        while parent:
            if isinstance(parent, DPDSLParser.AggregationContext):
                # Inside aggregation - already handled, skip
                return self.visitChildren(ctx)
            parent = parent.parentCtx if hasattr(parent, 'parentCtx') else None
        
        # Not in aggregation - process normally
        if ctx.label():
            label = ctx.label().getText()
            col_name = ctx.identifier().getText()
            
            if label == 'PUBLIC':
                # PUBLIC columns pass through
                self.rewriter.replaceRange(
                    ctx.start.tokenIndex, 
                    ctx.stop.tokenIndex, 
                    col_name
                )
            elif label == 'PRIVATE':
                # PRIVATE columns in SELECT (non-aggregated) should be BLOCKED
                self.errors.append(
                    f"ERROR: PRIVATE column '{col_name}' cannot be selected directly. "
                    f"Use aggregation with DP noise instead."
                )
        
        return self.visitChildren(ctx)
    
    def visitGroup_by_clause(self, ctx):
        """Check for PRIVATE columns in GROUP BY"""
        self.has_group_by = True
        
        # Check all columns in GROUP BY clause
        for col_ctx in ctx.groupByColumn():
            if col_ctx.label() and col_ctx.label().getText() == 'PRIVATE':
                col_name = col_ctx.identifier().getText()
                self.private_columns_in_groupby.append(col_name)
                self.errors.append(
                    f"SYNTAX ERROR: GROUP BY on PRIVATE column '{col_name}' is not allowed. "
                    f"PRIVATE columns cannot be used for grouping."
                )
                # Also rewrite to prevent double error from visitLabeledColumn
                self.rewriter.replaceRange(
                    col_ctx.start.tokenIndex,
                    col_ctx.stop.tokenIndex,
                    col_name
                )
            elif col_ctx.label() and col_ctx.label().getText() == 'PUBLIC':
                # Replace PUBLIC label with just column name in GROUP BY
                col_name = col_ctx.identifier().getText()
                self.rewriter.replaceRange(
                    col_ctx.start.tokenIndex,
                    col_ctx.stop.tokenIndex,
                    col_name
                )
        
        return self.visitChildren(ctx)

    def get_rewritten_sql(self):
        """Get final SQL - workaround for Python TokenStreamRewriter whitespace issue"""
        # Python's getDefaultText() strips whitespace - known limitation
        # Workaround: Insert spaces heuristically based on SQL syntax
        
        sql = self.rewriter.getDefaultText()
        
        # Clean up DPDSL syntax FIRST (before adding spaces)
        sql = re.sub(r'OF\s*\[[^\]]+\]', '', sql)
        sql = re.sub(r'\bPRIVATE\s*', '', sql)  # Remove PRIVATE with optional space
        sql = re.sub(r'\bPUBLIC\s*', '', sql)   # Remove PUBLIC with optional space
        
        # Insert spaces around SQL keywords
        keywords = ['SELECT', 'FROM', 'WHERE', 'GROUP', 'BY']
        for kw in keywords:
            # Space after keyword (except before opening paren)
            sql = re.sub(rf'{kw}(?![\(\s])', f'{kw} ', sql)
            # Space before keyword (except after opening paren or comma)
            sql = re.sub(rf'(?<![\(\s,]){kw}', f' {kw}', sql)
        
        # Space after comma
        sql = re.sub(r',(?!\s)', ', ', sql)
        
        # Space around operators (but not inside function calls)
        sql = re.sub(r'(\))(\+)', r'\1 \2 ', sql)
        
        # Normalize multiple spaces
        sql = re.sub(r'\s+', ' ', sql)
        
        return sql.strip()


# --- TEST DATABASE SETUP ---
def setup_test_db():
    """Create test database with known values"""
    conn = sqlite3.connect(":memory:")
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE employees (
            id INTEGER PRIMARY KEY,
            name TEXT,
            ssn TEXT,
            salary INTEGER,
            age INTEGER,
            department TEXT,
            address TEXT,
            medical_cost INTEGER
        )
    """)
    
    # Insert test data
    test_data = [
        (1, "Alice", "123-45-6789", 50000, 30, "Engineering", "123 Main St", 5000),
        (2, "Bob", "987-65-4321", 80000, 45, "Sales", "456 Oak Ave", 15000),
        (3, "Charlie", "555-55-5555", 120000, 50, "Engineering", "789 Pine Rd", 25000),
        (4, "Diana", "111-22-3333", 200000, 55, "Engineering", "321 Elm St", 100000),  # Outlier
        (5, "Eve", "999-88-7777", 1000000000, 60, "Executive", "999 CEO Blvd", 500000),  # Extreme outlier
    ]
    
    cursor.executemany(
        "INSERT INTO employees VALUES (?,?,?,?,?,?,?,?)",
        test_data
    )
    conn.commit()
    return conn


def view_test_data():
    """Display the test database contents"""
    print("\n" + "=" * 80)
    print("üìä TEST DATABASE CONTENTS")
    print("=" * 80)
    
    conn = setup_test_db()
    cursor = conn.cursor()
    
    cursor.execute("SELECT * FROM employees ORDER BY id")
    rows = cursor.fetchall()
    
    # Print header
    print(f"\n{'ID':<4} {'Name':<10} {'SSN':<15} {'Salary':<15} {'Age':<5} {'Department':<12} {'Address':<20} {'Medical':<10}")
    print("-" * 100)
    
    # Print rows
    for row in rows:
        id_val, name, ssn, salary, age, dept, addr, med = row
        print(f"{id_val:<4} {name:<10} {ssn:<15} {salary:<15,} {age:<5} {dept:<12} {addr:<20} {med:<10,}")
    
    # Print statistics
    print("\n" + "-" * 100)
    cursor.execute("SELECT AVG(salary), MAX(salary), MIN(salary) FROM employees")
    avg_sal, max_sal, min_sal = cursor.fetchone()
    print(f"üìà Statistics:")
    print(f"   Average Salary: ${avg_sal:,.2f}")
    print(f"   Max Salary: ${max_sal:,} (Eve - the outlier!)")
    print(f"   Min Salary: ${min_sal:,}")
    print(f"   ‚ö†Ô∏è  Notice Eve's salary would break a normal query without DP protection!")
    
    conn.close()
    print("=" * 80 + "\n")


# --- HELPER FUNCTION ---
from antlr4.error.ErrorListener import ErrorListener

# Custom error listener to catch syntax errors
class SyntaxErrorListener(ErrorListener):
    def __init__(self):
        super().__init__()
        self.errors = []
    
    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        self.errors.append(f"Syntax error at line {line}:{column} - {msg}")


def rewrite_and_execute(sql_input, conn, verbose=False, budget_manager=None):
    """Parse, rewrite, and execute a DPDSL query with optional budget tracking and JOIN support"""
    try:
        input_stream = InputStream(sql_input)
        lexer = DPDSLLexer(input_stream)
        
        # Add error listener to lexer
        lexer.removeErrorListeners()
        lexer_error_listener = SyntaxErrorListener()
        lexer.addErrorListener(lexer_error_listener)
        
        stream = CommonTokenStream(lexer)
        parser = DPDSLParser(stream)
        
        # Add error listener to parser
        parser.removeErrorListeners()
        parser_error_listener = SyntaxErrorListener()
        parser.addErrorListener(parser_error_listener)
        
        tree = parser.query()
        
        # Check for syntax errors from both lexer and parser
        all_syntax_errors = lexer_error_listener.errors + parser_error_listener.errors
        
        if all_syntax_errors:
            if verbose:
                print(f"   Parse failed with syntax errors")
            return None, all_syntax_errors
        
        # Also check parser's error count
        if parser.getNumberOfSyntaxErrors() > 0:
            if verbose:
                print(f"   Parse had {parser.getNumberOfSyntaxErrors()} syntax error(s)")
            return None, ["Syntax error: Keywords must be UPPERCASE (SELECT, FROM, PRIVATE, PUBLIC, etc.)"]
        
        # Create visitor with original query for JOIN analysis
        visitor = DPDSL_Rewriter(stream, original_query=sql_input)
        visitor.visit(tree)
        
        # Check for errors
        if visitor.errors:
            return None, visitor.errors
        
        # Check budget BEFORE executing query
        if budget_manager and visitor.privacy_cost > 0:
            if not budget_manager.check_affordability(visitor.privacy_cost):
                return None, [
                    f"‚õî INSUFFICIENT BUDGET: Query needs Œµ={visitor.privacy_cost:.2f}, "
                    f"but only Œµ={budget_manager.remaining():.2f} remains"
                ]
        
        final_sql = visitor.get_rewritten_sql()
        
        if verbose:
            print(f"   Rewritten SQL: {final_sql}")
            print(f"   Privacy cost: Œµ = {visitor.privacy_cost}")
            if visitor.has_join:
                print(f"   JOIN detected: Elastic sensitivity enabled")
                if visitor.join_path:
                    print(f"   Tables: {', '.join(visitor.join_path.tables)}")
                    print(f"   Primary entity: {visitor.join_path.primary_entity_table}")
            if budget_manager:
                print(f"   Budget remaining: Œµ = {budget_manager.remaining():.2f}")
        
        # Execute query
        # NEW: Handle JOINs with elastic clipping
        if visitor.has_join and visitor.join_path and ELASTIC_CONFIG['enabled']:
            # Execute JOIN to get full result set
            try:
                df = pd.read_sql_query(final_sql, conn)
                
                if verbose:
                    print(f"   Rows after JOIN: {len(df)}")
                
                # Apply elastic clipping
                df = visitor.elastic_manager.apply_elastic_clipping(
                    df,
                    visitor.join_path,
                    verbose=verbose
                )
                
                # For aggregation queries, compute the result from clipped dataframe
                # This is a simplified approach - production would need more sophisticated handling
                if len(df) > 0:
                    # Return as list of tuples to match cursor.fetchall() format
                    result = [(len(df),)]  # Simple COUNT for now
                else:
                    result = [(0,)]
                
            except Exception as e:
                return None, [f"Error executing JOIN query: {str(e)}"]
        else:
            # Regular execution (no JOIN or elastic sensitivity disabled)
            cursor = conn.cursor()
            cursor.execute(final_sql)
            result = cursor.fetchall()
        
        # Spend budget AFTER successful execution
        if budget_manager and visitor.privacy_cost > 0:
            budget_manager.spend(visitor.privacy_cost, sql_input)
            if verbose:
                print(f"   New budget remaining: Œµ = {budget_manager.remaining():.2f}")
        
        return result, None
    except ValueError as e:
        # Budget error
        return None, [str(e)]
    except Exception as e:
        # Don't print traceback for expected syntax errors
        error_msg = str(e)
        if "near" in error_msg and "syntax error" in error_msg:
            return None, [f"Syntax error: Keywords must be UPPERCASE (SELECT, FROM, PRIVATE, PUBLIC, etc.)"]
        
        import traceback
        if verbose:
            traceback.print_exc()
        return None, [f"Error: {error_msg}"]


# --- TEST SUITE ---
class TestDPDSL(unittest.TestCase):
    
    def setUp(self):
        """Create fresh database for each test"""
        self.conn = setup_test_db()
    
    def tearDown(self):
        """Close database connection"""
        self.conn.close()
    
    def test_public_column_passthrough(self):
        """PUBLIC columns like department should work without modification"""
        sql = "SELECT PUBLIC department FROM employees"
        result, errors = rewrite_and_execute(sql, self.conn)
        
        self.assertIsNone(errors, f"PUBLIC column should not produce errors: {errors}")
        self.assertIsNotNone(result, "PUBLIC column should return results")
        self.assertGreater(len(result), 0, "Should return at least one row")
    
    def test_private_aggregation_adds_noise(self):
        """MAX(PRIVATE salary) should clip at 150k and add Laplace noise"""
        sql = "SELECT MAX(PRIVATE salary OF [0.5]) FROM employees"
        
        # Run multiple times to see the pattern
        results = []
        for _ in range(10):
            result, errors = rewrite_and_execute(sql, self.conn)
            self.assertIsNone(errors, f"PRIVATE aggregation should not error: {errors}")
            self.assertIsNotNone(result)
            results.append(result[0][0])
        
        # Average should be around 150k (clipping prevents 1 billion from showing)
        avg_result = np.mean(results)
        
        # The key test: average should be MUCH less than 1 billion
        # (proving clipping worked) but around 150k range
        self.assertLess(avg_result, 1000000, 
                       f"Average {avg_result:,} should be clipped, not near 1 billion")
        
        # Results should vary (proving noise was added)
        variance = np.var(results)
        self.assertGreater(variance, 0, "Results should vary due to noise")
    
    def test_count_star(self):
        """COUNT(*) should work without privacy issues"""
        sql = "SELECT COUNT(*) FROM employees"
        result, errors = rewrite_and_execute(sql, self.conn)
        
        self.assertIsNone(errors, f"COUNT(*) should not error: {errors}")
        self.assertEqual(result[0][0], 5, "Should count all 5 employees")
    
    def test_epsilon_affects_noise_scale(self):
        """Lower epsilon should add more noise"""
        sql_high_eps = "SELECT AVG(PRIVATE salary OF [10.0]) FROM employees"
        sql_low_eps = "SELECT AVG(PRIVATE salary OF [0.1]) FROM employees"
        
        results_high = []
        results_low = []
        
        for _ in range(10):
            r_high, _ = rewrite_and_execute(sql_high_eps, self.conn)
            r_low, _ = rewrite_and_execute(sql_low_eps, self.conn)
            if r_high and r_low:
                results_high.append(r_high[0][0])
                results_low.append(r_low[0][0])
        
        if len(results_high) >= 5 and len(results_low) >= 5:
            variance_high = np.var(results_high)
            variance_low = np.var(results_low)
            
            self.assertGreater(variance_low, variance_high,
                              "Lower epsilon should produce higher variance (more noise)")
    
    def test_private_direct_select_blocked(self):
        """Selecting PRIVATE ssn directly should produce an error"""
        sql = "SELECT PRIVATE ssn FROM employees"
        result, errors = rewrite_and_execute(sql, self.conn)
        
        self.assertIsNotNone(errors, "Direct PRIVATE select should be blocked")
        self.assertIn("cannot be selected directly", errors[0])
    
    def test_groupby_private_blocked(self):
        """GROUP BY PRIVATE salary should produce syntax error"""
        sql = "SELECT COUNT(*) FROM employees GROUP BY PRIVATE salary"
        
        input_stream = InputStream(sql)
        lexer = DPDSLLexer(input_stream)
        stream = CommonTokenStream(lexer)
        parser = DPDSLParser(stream)
        tree = parser.query()
        
        visitor = DPDSL_Rewriter(stream)
        visitor.visit(tree)
        
        # Should have at least one error about GROUP BY
        self.assertTrue(len(visitor.errors) > 0, 
                       "GROUP BY on PRIVATE should produce error")
        
        # Check that the error mentions GROUP BY
        has_groupby_error = any("GROUP BY" in err for err in visitor.errors)
        self.assertTrue(has_groupby_error, 
                       f"Should have GROUP BY error, got: {visitor.errors}")
    
    def test_groupby_public_allowed(self):
        """GROUP BY PUBLIC department should work fine"""
        sql = "SELECT PUBLIC department, COUNT(*) FROM employees GROUP BY PUBLIC department"
        result, errors = rewrite_and_execute(sql, self.conn)
        
        self.assertIsNone(errors, f"GROUP BY PUBLIC should be allowed: {errors}")
        self.assertIsNotNone(result)
        self.assertGreater(len(result), 0, "Should return grouped results")
    
    def test_mixed_public_private(self):
        """Can mix PUBLIC and PRIVATE columns correctly"""
        sql = "SELECT PUBLIC department, AVG(PRIVATE salary OF [1.0]) FROM employees GROUP BY PUBLIC department"
        result, errors = rewrite_and_execute(sql, self.conn)
        
        self.assertIsNone(errors, f"Mixed PUBLIC/PRIVATE should work: {errors}")
        self.assertIsNotNone(result)
    
    def test_clipping_prevents_outliers(self):
        """Verify that outliers are actually clipped"""
        sql = "SELECT MAX(PRIVATE salary OF [100.0]) FROM employees"
        
        results = []
        for _ in range(20):
            result, _ = rewrite_and_execute(sql, self.conn)
            if result:
                results.append(result[0][0])
        
        if len(results) >= 10:
            avg_result = np.mean(results)
            
            self.assertLess(avg_result, 180000, 
                           f"Average result {avg_result} should be near 150k bound")
            self.assertGreater(avg_result, 120000,
                              f"Average result {avg_result} should be near 150k bound")
    
    def test_multiple_private_columns(self):
        """Can use multiple PRIVATE columns with different bounds"""
        sql = "SELECT AVG(PRIVATE salary OF [1.0]), AVG(PRIVATE age OF [1.0]) FROM employees"
        result, errors = rewrite_and_execute(sql, self.conn)
        
        self.assertIsNone(errors, f"Multiple PRIVATE columns should work: {errors}")
        self.assertIsNotNone(result)
        if result:
            self.assertEqual(len(result[0]), 2, "Should return two aggregated values")
    
    # === EDGE CASE TESTS ===
    
    def test_complex_math_expressions(self):
        """Test multiple PRIVATE aggregations in arithmetic expressions"""
        sql = "SELECT AVG(PRIVATE salary OF [1.0]) FROM employees"
        result1, errors1 = rewrite_and_execute(sql, self.conn)
        
        sql2 = "SELECT AVG(PRIVATE medical_cost OF [1.0]) FROM employees"
        result2, errors2 = rewrite_and_execute(sql2, self.conn)
        
        # Both should work independently
        self.assertIsNone(errors1, f"First PRIVATE aggregation should work: {errors1}")
        self.assertIsNone(errors2, f"Second PRIVATE aggregation should work: {errors2}")
        
        # Note: Complex expressions like SUM(PRIVATE x) + SUM(PRIVATE y) in single query
        # would require more advanced parsing. This tests that multiple separate queries work.
    
    def test_very_low_epsilon(self):
        """Test with very low epsilon (high noise) budget"""
        sql = "SELECT MAX(PRIVATE salary OF [0.0001]) FROM employees"
        
        results = []
        for _ in range(5):
            result, errors = rewrite_and_execute(sql, self.conn)
            self.assertIsNone(errors, f"Very low epsilon should still work: {errors}")
            if result:
                results.append(result[0][0])
        
        # With very low epsilon, noise should be VERY high
        if len(results) >= 3:
            variance = np.var(results)
            # Sensitivity/epsilon = 150000/0.0001 = 1,500,000,000 scale!
            # Variance should be massive
            self.assertGreater(variance, 1000000, 
                             f"Very low epsilon should produce huge variance: {variance}")
    
    def test_budget_manager(self):
        """Test that budget manager tracks and enforces privacy budget"""
        budget = BudgetManager(max_budget=2.0)
        
        # First query: costs 1.0, should succeed
        sql1 = "SELECT AVG(PRIVATE salary OF [1.0]) FROM employees"
        result1, errors1 = rewrite_and_execute(sql1, self.conn, budget_manager=budget)
        self.assertIsNone(errors1, "First query should succeed")
        self.assertAlmostEqual(budget.remaining(), 1.0, places=2)
        
        # Second query: costs 0.5, should succeed
        sql2 = "SELECT MAX(PRIVATE age OF [0.5]) FROM employees"
        result2, errors2 = rewrite_and_execute(sql2, self.conn, budget_manager=budget)
        self.assertIsNone(errors2, "Second query should succeed")
        self.assertAlmostEqual(budget.remaining(), 0.5, places=2)
        
        # Third query: costs 1.0, should FAIL (exceeds budget)
        sql3 = "SELECT AVG(PRIVATE medical_cost OF [1.0]) FROM employees"
        result3, errors3 = rewrite_and_execute(sql3, self.conn, budget_manager=budget)
        self.assertIsNotNone(errors3, "Third query should fail due to budget")
        self.assertIn("BUDGET", errors3[0] if errors3 else "")
        
        # Verify budget unchanged after failed query
        self.assertAlmostEqual(budget.remaining(), 0.5, places=2)


# --- MANUAL TEST RUNNER ---
def run_manual_tests():
    """Run some visual tests with output"""
    print("=" * 80)
    print("üîí DPDSL SQL REWRITER - MANUAL TEST OUTPUT")
    print("=" * 80)
    
    conn = setup_test_db()
    
    tests = [
        ("PUBLIC Pass-Through", 
         "SELECT PUBLIC department FROM employees"),
        
        ("COUNT(*) - Basic Aggregation",
         "SELECT COUNT(*) FROM employees"),
        
        ("PRIVATE Aggregation with Noise", 
         "SELECT MAX(PRIVATE salary OF [0.5]) FROM employees"),
        
        ("PRIVATE Direct Select (Should Block)", 
         "SELECT PRIVATE ssn FROM employees"),
        
        ("GROUP BY PRIVATE (Should Block)", 
         "SELECT COUNT(*) FROM employees GROUP BY PRIVATE address"),
        
        ("GROUP BY PUBLIC (Should Work)", 
         "SELECT PUBLIC department, COUNT(*) FROM employees GROUP BY PUBLIC department"),
        
        ("Mixed PUBLIC and PRIVATE",
         "SELECT PUBLIC department, AVG(PRIVATE salary OF [1.0]) FROM employees GROUP BY PUBLIC department"),
        
        # Edge cases
        ("EDGE: Very Low Epsilon (High Noise)",
         "SELECT MAX(PRIVATE salary OF [0.0001]) FROM employees"),
        
        ("EDGE: Multiple PRIVATE Aggregations",
         "SELECT AVG(PRIVATE salary OF [1.0]), AVG(PRIVATE medical_cost OF [1.0]) FROM employees"),
    ]
    
    for test_name, sql in tests:
        print(f"\n{'‚îÄ' * 80}")
        print(f"üìù TEST: {test_name}")
        print(f"üîπ INPUT: {sql}")
        
        result, errors = rewrite_and_execute(sql, conn, verbose=True)
        
        if errors:
            # Check if this is an expected error (security blocking)
            if "Should Block" in test_name:
                print(f"üîí BLOCKED (Expected):")
                for err in errors:
                    print(f"   {err}")
            else:
                print(f"‚ùå ERRORS:")
                for err in errors:
                    print(f"   {err}")
        else:
            print(f"‚úÖ SUCCESS")
            print(f"üìä RESULT: {result}")
    
    conn.close()
    print("\n" + "=" * 80)


def demo_budget_manager():
    """Demonstrate the privacy budget manager"""
    print("\n" + "=" * 80)
    print("üí∞ PRIVACY BUDGET MANAGER DEMO")
    print("=" * 80)
    print("Simulating an analyst with Œµ=3.0 total budget\n")
    
    conn = setup_test_db()
    budget = BudgetManager(max_budget=3.0)
    
    queries = [
        ("Query 1: Average salary", "SELECT AVG(PRIVATE salary OF [1.0]) FROM employees"),
        ("Query 2: Max age", "SELECT MAX(PRIVATE age OF [0.5]) FROM employees"),
        ("Query 3: Average medical cost", "SELECT AVG(PRIVATE medical_cost OF [1.0]) FROM employees"),
        ("Query 4: Count employees", "SELECT COUNT(*) FROM employees"),  # Free (no privacy cost)
        ("Query 5: Max salary (should FAIL)", "SELECT MAX(PRIVATE salary OF [1.0]) FROM employees"),
    ]
    
    for i, (desc, sql) in enumerate(queries, 1):
        print(f"\n{'‚îÄ'*80}")
        print(f"üîπ {desc}")
        print(f"   SQL: {sql}")
        print(f"   Budget before: Œµ = {budget.remaining():.2f}")
        
        result, errors = rewrite_and_execute(sql, conn, verbose=False, budget_manager=budget)
        
        if errors:
            print(f"   ‚ùå {errors[0]}")
        else:
            print(f"   ‚úÖ Success! Result: {result[0] if result else 'N/A'}")
        
        print(f"   Budget after: Œµ = {budget.remaining():.2f}")
    
    print(f"\n{budget.get_report()}")
    conn.close()


def run_security_audit():
    """
    Security Audit: Test DPDSL against realistic attack scenarios
    This simulates what a malicious Meta employee might try
    """
    
    print("DPDSL SECURITY AUDIT: Employee Attack Scenarios")
    print("Testing what happens when employees try to extract sensitive data")

    
    results = {}
    
    # ============================================================================
    # ATTACK 1: Direct PII Extraction
    # ============================================================================

    print("ATTACK TYPE 1: DIRECT PII EXTRACTION")

    
    print("\nüéØ Attack 1.1: Extract SSNs")
    print("‚îÄ"*80)
    print("Employee query: SELECT PRIVATE ssn FROM employees")
    conn = setup_test_db()
    result, errors = rewrite_and_execute("SELECT PRIVATE ssn FROM employees", conn, verbose=False)
    if errors:
        print("‚úÖ BLOCKED:", errors[0])
        results['ssn_extraction'] = 'BLOCKED'
    else:
        print("‚ùå ALLOWED - Data leaked:", result[:2])
        results['ssn_extraction'] = 'VULNERABLE'
    conn.close()
    
    print("\nüéØ Attack 1.2: Extract Names + Salaries")
    print("‚îÄ"*80)
    print("Employee query: SELECT PRIVATE name, PRIVATE salary FROM employees")
    conn = setup_test_db()
    result, errors = rewrite_and_execute("SELECT PRIVATE name, PRIVATE salary FROM employees", conn, verbose=False)
    if errors:
        print("‚úÖ BLOCKED:", errors[0])
        results['name_salary'] = 'BLOCKED'
    else:
        print("‚ùå ALLOWED - Data leaked:", result[:2])
        results['name_salary'] = 'VULNERABLE'
    conn.close()
    
    # ============================================================================
    # ATTACK 2: Aggregation with Filtering (Narrowing)
    # ============================================================================
 
    print("ATTACK TYPE 2: NARROWING VIA FILTERS")
  
    
    print("\nüéØ Attack 2.1: Average salary of just executives (small group)")
    print("‚îÄ"*80)
    print("Employee query: SELECT AVG(PRIVATE salary OF [1.0]) FROM employees WHERE PUBLIC department = 'Executive'")
    conn = setup_test_db()
    result, errors = rewrite_and_execute(
        "SELECT AVG(PRIVATE salary OF [1.0]) FROM employees WHERE PUBLIC department = 'Executive'", 
        conn, verbose=False
    )
    if errors:
        print("‚úÖ BLOCKED:", errors[0])
        results['narrowing'] = 'BLOCKED'
    else:
        # In test DB, Executive has only 1 person (Eve) - this should be noisy
        print("‚ö†Ô∏è  ALLOWED (with noise):", result)
        print("    Note: Only 1 Executive in DB - noise protects individual value")
        results['narrowing'] = 'PROTECTED'
    conn.close()
    
    # ============================================================================
    # ATTACK 3: GROUP BY to Create Small Groups
    # ============================================================================
    
    print("ATTACK TYPE 3: GROUP BY ATTACKS")

    
    print("\nüéØ Attack 3.1: GROUP BY home address (unique per person)")
    print("‚îÄ"*80)
    print("Employee query: SELECT PRIVATE address, AVG(PRIVATE salary OF [1.0]) FROM employees GROUP BY PRIVATE address")
    conn = setup_test_db()
    result, errors = rewrite_and_execute(
        "SELECT PRIVATE address, AVG(PRIVATE salary OF [1.0]) FROM employees GROUP BY PRIVATE address",
        conn, verbose=False
    )
    if errors:
        print("‚úÖ BLOCKED:", errors[0])
        results['groupby_address'] = 'BLOCKED'
    else:
        print("‚ùå ALLOWED - Could reveal individual salaries:", result[:2])
        results['groupby_address'] = 'VULNERABLE'
    conn.close()
    
    print("\nüéØ Attack 3.2: GROUP BY department (PUBLIC, legitimate)")
    print("‚îÄ"*80)
    print("Employee query: SELECT PUBLIC department, AVG(PRIVATE salary OF [1.0]) FROM employees GROUP BY PUBLIC department")
    conn = setup_test_db()
    result, errors = rewrite_and_execute(
        "SELECT PUBLIC department, AVG(PRIVATE salary OF [1.0]) FROM employees GROUP BY PUBLIC department",
        conn, verbose=False
    )
    if errors:
        print("‚ùå FALSE POSITIVE - Legitimate query blocked:", errors[0])
        results['groupby_public'] = 'FALSE_POSITIVE'
    else:
        print("‚úÖ ALLOWED (legitimate analytics):", result[:2])
        results['groupby_public'] = 'ALLOWED'
    conn.close()
    
    # ============================================================================
    # ATTACK 4: Composition Attack (Repeated Queries)
    # ============================================================================
  
    print("ATTACK TYPE 4: COMPOSITION ATTACK (Averaging out noise)")

    
    print("\nüéØ Attack 4: Run same query 100 times to reduce noise")
    print("‚îÄ"*80)
    print("Without budget manager:")
    conn = setup_test_db()
    salaries = []
    for i in range(10):
        result, _ = rewrite_and_execute(
            "SELECT AVG(PRIVATE salary OF [0.1]) FROM employees",
            conn, verbose=False
        )
        if result:
            salaries.append(result[0][0])
    
    avg_of_noisy = np.mean(salaries)
    variance = np.var(salaries)
    print(f"    10 queries completed")
    print(f"    Average of results: ${avg_of_noisy:,.2f}")
    print(f"    Variance: {variance:,.2f}")
    print(f"    ‚ùå VULNERABLE: Attacker can average out noise with unlimited queries")
    results['composition_no_budget'] = 'VULNERABLE'
    conn.close()
    
    print("\nWith budget manager:")
    budget = BudgetManager(max_budget=1.0)
    conn = setup_test_db()
    success_count = 0
    for i in range(20):
        result, errors = rewrite_and_execute(
            "SELECT AVG(PRIVATE salary OF [0.1]) FROM employees",
            conn, verbose=False, budget_manager=budget
        )
        if not errors:
            success_count += 1
        else:
            print(f"    Query {i+1}: Budget exhausted")
            print(f"        {errors[0]}")  # Show full error message
            break
    
    print(f"    Queries succeeded: {success_count}/20")
    print(f"    ‚úÖ PROTECTED: Budget manager stopped attack")
    results['composition_with_budget'] = 'PROTECTED'
    conn.close()
    
    # ============================================================================
    # ATTACK 5: Differencing Attack
    # ============================================================================
 
    print("ATTACK TYPE 5: DIFFERENCING ATTACK")
 
    
    print("\nüéØ Attack 5: Infer Alice's salary via subtraction")
    print("‚îÄ"*80)
    print("Query 1: SUM of all salaries")
    print("Query 2: SUM of all EXCEPT Alice")
    print("Difference = Alice's salary (if noise is insufficient)")
    
    conn = setup_test_db()
    
    # Query 1
    r1, _ = rewrite_and_execute("SELECT SUM(PRIVATE salary OF [1.0]) FROM employees", conn, verbose=False)
    total_all = r1[0][0] if r1 else 0
    
    # Query 2 - Note: this might not work with current grammar (no WHERE with != )
    # For demo, we'll note the limitation
    print(f"    Total all: ${total_all:,.2f}")
    print("    ‚ö†Ô∏è  NOTE: Current DPDSL doesn't support WHERE with !=")
    print("    This attack vector needs: WHERE clause support in grammar")
    print("    Status: Limited grammar provides some protection")
    results['differencing'] = 'PARTIALLY_PROTECTED'
    conn.close()
    
    # ============================================================================
    # SUMMARY
    # ============================================================================
    print("\n\n" + "="*80)
    print("üìä SECURITY AUDIT SUMMARY")
    print("="*80)
    
    blocked = sum(1 for v in results.values() if v == 'BLOCKED')
    protected = sum(1 for v in results.values() if v in ['PROTECTED', 'PARTIALLY_PROTECTED'])
    vulnerable = sum(1 for v in results.values() if v == 'VULNERABLE')
    
    print(f"\nüõ°Ô∏è  Attacks Blocked: {blocked}")
    print(f"‚ö†Ô∏è  Attacks Protected (with noise/budget): {protected}")
    print(f"üî¥ Vulnerabilities: {vulnerable}")
    
    print(f"\n{'Attack Type':<40} {'Status':<20}")
    print("‚îÄ"*60)
    for name, status in results.items():
        icon = '‚úÖ' if status in ['BLOCKED', 'PROTECTED', 'ALLOWED', 'PARTIALLY_PROTECTED'] else '‚ùå'
        print(f"{icon} {name:<38} {status:<20}")
    
    print("\n" + "="*80)
    print("VERDICT:")
    if vulnerable == 0:
        print("üü¢ EXCELLENT: All attacks blocked or properly mitigated")
        print("   Ready for internal deployment with budget manager enabled")
    elif vulnerable <= 2:
        print("üü° GOOD START: Core protections working")
        print("   Recommendations:")
        print("   1. Always use BudgetManager in production")
        print("   2. Add query result size limits")
        print("   3. Enable audit logging")
    else:
        print("üî¥ NEEDS WORK: Multiple vulnerabilities found")
        print("   Do not deploy to production yet")
    print("="*80)


    print("="*80)


def demo_multi_table_joins():
    """Demonstrate multi-table JOIN support with elastic sensitivity"""

    print("MULTI-TABLE JOIN DEMONSTRATION")
    conn = sqlite3.connect(":memory:")
    
    # Employees
    employees_df = pd.DataFrame({
        'id': [1, 2, 3],
        'name': ['Alice', 'Bob', 'Charlie'],
        'salary': [80000, 90000, 120000],
    })
    employees_df.to_sql('employees', conn, index=False, if_exists='replace')
    
    # Projects (Alice on 3 projects, Bob on 2, Charlie on 1)
    projects_df = pd.DataFrame({
        'id': [1, 2, 3, 4, 5, 6],
        'employee_id': [1, 1, 1, 2, 2, 3],
        'name': ['Project A', 'Project B', 'Project C', 'Project D', 'Project E', 'Project F'],
        'budget': [10000, 15000, 20000, 25000, 30000, 35000]
    })
    projects_df.to_sql('projects', conn, index=False, if_exists='replace')
    
    print("üìä Database Contents:")
    print("="*80)
    
    # Show Employees table
    print("\n1Ô∏è‚É£ EMPLOYEES TABLE:")
    print(f"{'ID':<5} {'Name':<10} {'Salary':<15}")
    print("-"*35)
    for _, row in employees_df.iterrows():
        print(f"{row['id']:<5} {row['name']:<10} ${row['salary']:,}")
    
    # Show Projects table
    print("\n2Ô∏è‚É£ PROJECTS TABLE:")
    print(f"{'ID':<5} {'Employee ID':<15} {'Project Name':<15} {'Budget':<15}")
    print("-"*55)
    for _, row in projects_df.iterrows():
        print(f"{row['id']:<5} {row['employee_id']:<15} {row['name']:<15} ${row['budget']:,}")
    
    # Show JOIN analysis
    print("\nüìà JOIN Analysis:")
    print("-"*80)
    print(f"   Total employees: {len(employees_df)}")
    print(f"   Total projects: {len(projects_df)}")
    print(f"   After JOIN: {len(projects_df)} rows (employee data duplicated)")
    print("\n   Contribution breakdown:")
    contrib = projects_df.groupby('employee_id').size()
    for emp_id, count in contrib.items():
        emp_name = employees_df[employees_df['id'] == emp_id]['name'].values[0]
        print(f"      {emp_name} (id={emp_id}): {count} contributions")
    
    print("\n‚ö†Ô∏è  Problem: Alice appears 3√ó in the result!")
    print("   Her salary will be counted 3 times in AVG calculation")
    print("   This gives her 3√ó influence on the result")
    
    print("\n" + "="*80)
    print("Query: Average salary across employee-project JOIN")
    print("="*80)
    print("SQL: SELECT AVG(PRIVATE e.salary OF [1.0])")
    print("     FROM employees e")
    print("     JOIN projects p ON e.id = p.employee_id")
    
    print("\n‚ö†Ô∏è  Without elastic sensitivity:")
    print("   Alice's salary counted 3 times (3 projects)")
    print("   Sensitivity = 3 √ó $150,000 = $450,000")
    print("   Noise will be very large!")
    
    print("\n‚úÖ With elastic sensitivity (max_contributions=3):")
    print("   Alice's contributions clipped to 3 (no change in this case)")
    print("   But if she had 10 projects, only 3 would count!")
    print("   Sensitivity = 3 √ó $150,000 = $450,000 (controlled)")
    
    print(f"\nüí° Elastic sensitivity is {'ENABLED' if ELASTIC_CONFIG['enabled'] else 'DISABLED'}")
    print(f"   Max contributions per person: {ELASTIC_CONFIG['max_contributions']}")
    print(f"   Configure in ELASTIC_CONFIG dict at top of file")
    
    # Show what would happen with more projects
    print("\n" + "="*80)
    print("üéØ Scenario: What if Alice had 10 projects?")
    print("="*80)
    print("   Without elastic sensitivity:")
    print("      Sensitivity = 10 √ó $150,000 = $1,500,000")
    print("      Noise scale would be HUGE")
    print("\n   With elastic sensitivity (max=3):")
    print("      Only 3 of Alice's 10 projects counted")
    print("      Sensitivity = 3 √ó $150,000 = $450,000")
    print("      67% reduction in noise! üéâ")
    
    conn.close()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='DPDSL SQL Rewriter Testing')
    parser.add_argument('--mode', choices=['test', 'security', 'all'], default='all',
                       help='Run mode: test (unit tests), security (attack scenarios), all (both)')
    args = parser.parse_args()
    
    if args.mode in ['test', 'all']:
        # Show the test database contents first
        view_test_data()
        
        # Run unittest suite
        print("Running automated test suite...\n")
        unittest.main(argv=[''], verbosity=2, exit=False)
        
        # Run manual tests
        print("\n\n")
        run_manual_tests()
        
        # Demo budget manager
        demo_budget_manager()
    
    # Demo JOINs (if enabled)
    if args.mode in ['all']:
        demo_multi_table_joins()
    
    if args.mode in ['security', 'all']:
        # Run security audit
        print("\n\n")
        run_security_audit()